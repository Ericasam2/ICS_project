 # 1. LIF Neuron 

## a. objectives:
* fundamentals of the leaky integrate-and-fire (LIF) neuron 
    
## b. LIF Neurons characteristics: 
* Sum of weighted input
* integrate the input overtime with a leakage
spike -- when integration exceeds the threshold
* the Information is stored in the frequency of spikes

## c. Derivation:
* From the perspective of RC circuit
* ![Solution](https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_4_RCmembrane.png?raw=true)

## d. Integration:
* Using Euler method to solve the ODE
* $U(t+\Delta t) = U(t) + \frac{\Delta t}{\tau}\big(-U(t) + RI_{\rm in}(t)\big)$

## e. Inputs to Neuron:
*   step input: 
    *   $U_{\rm mem}(t)=I_{\rm in}(t)R [1 - e^{-\frac{t}{\tau}}]$
    *   ![Step input](https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/_static/lapicque_step.png?raw=true)
    *    finally  converge to $I_{\rm in}R$
*   pulse input :
membrane potential will jump straight up in virtually zero rise time

## f. Output:
*   Firing: If the membrane potential exceeds this threshold, then a voltage spike will be generated, external to the passive membrane model
*   reset mechanism: 
    *  each time a neuron fires, the membrane potential hyperpolarizes back to its resting potential 
    *   reset by subtraction (default) 
subtract the threshold from the membrane potential each time a spike is generated

#
# 2.  Feedforward Spiking NN
## a. Objectives: 
* Implement LIF in DL

## b. Characteristics:
* Decay rate: $U(t+\Delta t) = \beta U(t) $
* Weighted input: 
    *   $U[t+1] = \beta U[t] + WX[t+1] $
    *   W is a learnable parameter in DL 

## c. Spiking and Reset
* $U[t+1] = \underbrace{\beta U[t]}_\text{decay} + \underbrace{WX[t+1]}_\text{input} - \underbrace{S[t]U_{\rm thr}}_\text{reset} $

## d. Spiking Neural Network
* Input format: [$time \times batch\_size \times feature\_dimensions$]
* **spiking neurons can be treated like time-varying activation functions.**
*  We scale the input current with a weight generated by fully-connected layer

#
# 3. Training SNN
## a. Objectives:
*   Implement spiking neurons in recurrent network
*   Back propagation through time
*   Training SNN

## b. A RECURRENT REPRESENTATION OF SNNS
* Illustration: 
![Representation](https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial5/unrolled_2.png?raw=true)
* t: time-steps → The more time steps that are simulated, the deeper the graph becomes
* W: the weight of the inputs
* S: if the membrane potential exceeds the threshold, a spike is emitted

## c. Differentiable:
* Problems:
    * dead neuron problem: The derivative of the Heaviside step function from is the Dirac Delta function, which evaluates to 0 everywhere
* swap the derivative term $\partial S/\partial U$ for something that does not kill the learning process during the backward pass
* backward-pass derivative: $\frac{\partial \tilde{S}}{\partial U} \leftarrow \frac{1}{π}\frac{1}{(1+[Uπ]^2)}$
* ![BACKPROP THROUGH TIME](https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial5/bptt.png?raw=true)

## d. Output decoding:
* Rate decoding: take the neuron with the highest firing rate (or spike count) as the predicted class
    *  we want the correct neuron class to emit the most spikes over the course of the simulation run
    * The practical effect is that the membrane potential of the correct class is encouraged to increase while those of incorrect classes are reduced
    * CrossEntropy: $\mathcal{L}_{CE}[t] = -\sum_{i=0}^Cy_i{\rm log}(p_i[t]) \tag{9}$
* Latency decoding: Take the neuron that fires first as the predicted class

## e. Training:
* Accuracy: counts up all the spikes from each neuron (i.e., a rate code over the simulation time), and compares the index of the highest count with the actual target
* Loss: nn.CrossEntropyLoss